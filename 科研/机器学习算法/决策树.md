# 什么是决策树？

决策树是一种非参数的监督学习算法，以树状结构表示决策和决策可能的结果。它是机器学习中最常用的算法之一，既可用于分类也可用于回归问题。

# 基本概念

决策树模型是一个树状结构，包含以下组件：

1. **根节点**：包含整个数据集，是决策树的起点
2. **内部节点**：代表特征或属性的测试条件
3. **分支/边**：表示测试条件的结果，连接节点
4. **叶节点**：表示决策结果或预测值，不再分裂

# 工作原理

决策树通过递归方式构建：

1. 从根节点开始，选择最佳特征进行分裂
2. 根据特征值将数据集分成子集
3. 对每个子集重复此过程，直到满足停止条件
4. 在叶节点处做出最终决策或预测

# 分裂标准

决策树使用不同的度量方法来选择最佳分裂特征：

## 分类决策树

**信息增益**：基于信息熵减少的程度

- 熵表示数据集的混乱度，熵越高，数据越混乱
- 信息增益 = 父节点熵 - 子节点熵的加权和

**基尼不纯度**：衡量节点的不纯净程度

- 基尼值低表示节点中样本属于同一类的概率高
- CART算法常用此标准

**增益比**：信息增益除以特征的固有信息，避免偏向多值特征

- C4.5算法使用此标准

### 基尼指数

基尼指数(Gini Index)是决策树算法中常用的一种不纯度度量标准，特别是在CART(Classification and Regression Trees)算法中广泛应用。它衡量一个节点的样本纯度，用于确定最佳的分裂特征和分裂点。

#### 基本概念

基尼指数测量数据集中的不纯度或多样性。对于分类问题，基尼指数越小表示数据集中的样本越纯净(属于同一类别的可能性越大)。

#### 数学定义

假设数据集D中共有K个类别，第k类样本所占比例为pk，则基尼指数定义为：

Gini(D) = 1 - ∑(k=1 to K) pk²

对于二分类问题(K=2)，简化为： Gini(D) = 1 - p₁² - p₂² = 2p₁p₂

其中p₁和p₂分别是两个类别在数据集中的比例。

#### 在决策树中的应用

当选择特征A将数据集D分割为D₁和D₂两个子集时，分割后的基尼指数计算为：

Gini_index(D, A) = |D₁|/|D| × Gini(D₁) + |D₂|/|D| × Gini(D₂)

决策树算法选择能够使Gini_index最小的特征及其分割点作为最佳分裂。

#### 示例

假设有一个包含10个样本的数据集，其中7个属于类别A，3个属于类别B：

1. 计算原始基尼指数：
   - Gini(D) = 1 - (7/10)² - (3/10)² = 1 - 0.49 - 0.09 = 0.42
2. 假设某特征将数据集分成两部分：
   - D₁：5个样本，其中4个A类，1个B类
   - D₂：5个样本，其中3个A类，2个B类
3. 计算分割后的基尼指数：
   - Gini(D₁) = 1 - (4/5)² - (1/5)² = 1 - 0.64 - 0.04 = 0.32
   - Gini(D₂) = 1 - (3/5)² - (2/5)² = 1 - 0.36 - 0.16 = 0.48
   - Gini_index(D, A) = (5/10) × 0.32 + (5/10) × 0.48 = 0.16 + 0.24 = 0.40

由于分割后的基尼指数(0.40)小于原始基尼指数(0.42)，这表明该分割有助于提高数据纯度。

## 回归决策树

**方差减少**：选择可以最大程度减少子节点方差的特征

**平均绝对误差**：选择可以最小化预测值与真实值之间绝对差的特征

# 算法类型

几种常见的决策树算法：

- **ID3**：使用信息增益选择特征，不能处理连续值
- **C4.5**：ID3的改进版，使用增益比选择特征，可处理连续值和缺失值
- **CART**：使用基尼不纯度，构建二叉树，适用于分类和回归
- **CHAID**：使用卡方检验，可构建多叉树

# 剪枝技术

为了防止过拟合，决策树使用剪枝技术：

1. 预剪枝：
   - 在构建过程中提前停止树的生长
   - 设置最小样本数、最大深度等限制条件
2. 后剪枝：
   - 先构建完整树，再移除不重要的子树
   - 基于验证集性能评估是否剪枝