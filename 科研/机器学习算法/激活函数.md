# 什么是激活函数？

在神经网络中，**激活函数 (Activation Function)** 是一个至关重要的组成部分。它被应用于神经网络中每个神经元的输出，以决定该神经元是否应该被“激活”或者说，它的输出应该是什么。

# 为什么需要激活函数？

如果没有激活函数，神经网络的每一层都只是一个线性变换（矩阵乘法加上偏置）。无论神经网络有多少层，最终的输出仍然只是输入的线性组合。这意味着神经网络的表达能力会非常有限，无法学习和表示复杂的非线性关系。

**激活函数引入了非线性因素，使得神经网络能够学习和逼近任意复杂的函数，从而使其能够解决更复杂的任务，例如图像识别、自然语言处理等。**

# 激活函数的作用

**引入非线性：** 这是最核心的作用。非线性激活函数使得神经网络能够学习和表示非线性关系，从而处理更复杂的数据模式。

**限制输出范围：** 某些激活函数可以将神经元的输出限制在一个特定的范围内，例如 Sigmoid 函数将输出限制在 (0, 1)，Tanh 函数将输出限制在 (-1, 1)。这有助于稳定网络的训练过程。

**控制神经元的激活状态：** 激活函数的输出可以决定神经元是否应该被激活。例如，ReLU 函数在输入小于 0 时输出 0，相当于使该神经元“关闭”。

# 常见的激活函数及其特点

**Sigmoid 函数 (Logistic Function):**

- **数学形式：** $σ(x)=\frac{1}{1+e^{-z}}$
- **输出范围：** (0, 1)
- **特点：** 将输入压缩到 0 和 1 之间，可以解释为概率。在早期神经网络中广泛使用。
- 缺点：
  - **梯度消失 (Vanishing Gradient):** 当输入值很大或很小时，梯度接近于 0，导致反向传播时梯度无法有效地传递到前面的层，使得网络难以训练。
  - **输出不是以零为中心 (Not Zero-Centered):** 这可能导致梯度更新时出现 zigzagging 的现象，影响收敛速度。
  - **计算成本较高：** 涉及指数运算。

**双曲正切函数 (Tanh, Hyperbolic Tangent):**

- **数学形式：** $tanh(x)=\frac{e^x-e^{-x}}{e^x + e^{-x}}$
- **输出范围：** (-1, 1)
- **特点：** 将输入压缩到 -1 和 1 之间，输出以零为中心，缓解了 Sigmoid 函数的非零中心问题。
- **缺点：** 仍然存在梯度消失的问题，尤其是在输入值很大或很小时。计算成本也较高。

**修正线性单元 (ReLU, Rectified Linear Unit):**

- **数学形式：** $f(x)=max(0,x)$
- **输出范围：** [0, +∞)
- **特点：** 当输入大于 0 时，输出等于输入；当输入小于等于 0 时，输出为 0。计算简单高效。
- 优点：
  - **解决了梯度消失问题 (对于正输入)：** 在正区间梯度为常数 1，反向传播时梯度可以有效地传递。
  - **计算速度快：** 只需要进行简单的比较和取最大值操作。
  - **稀疏激活：** 使一部分神经元的输出为 0，有助于网络学习稀疏表示。
- 缺点：
  - **死亡 ReLU (Dying ReLU):** 如果一个神经元的输入长时间为负，其梯度将永远为 0，导致该神经元不再被激活，无法学习。
  - **输出不是以零为中心。**

**带泄露修正线性单元 (Leaky ReLU):**

- **数学形式：** ![image-20250417155217443](/Users/lin/Library/Application Support/typora-user-images/image-20250417155217443.png) ，其中 α 是一个很小的常数 (例如 0.01)。
- **输出范围：** (-∞, +∞)
- **特点：** 解决了 ReLU 的死亡问题，当输入为负时，输出一个很小的负斜率，而不是完全为 0。
- **优点：** 缓解了死亡 ReLU 问题，通常比 ReLU 表现更好。
- **缺点：** α 的选择可能影响性能，并且不如 ReLU 那么常用。

**参数化修正线性单元 (PReLU, Parametric ReLU):**

- **数学形式：** ![image-20250417155233471](/Users/lin/Library/Application Support/typora-user-images/image-20250417155233471.png)，其中 α 是一个可学习的参数，在训练过程中进行优化。
- **输出范围：** (-∞, +∞)
- **特点：** 与 Leaky ReLU 类似，但 α 是可学习的，可以自适应地调整负斜率。
- **优点：** 可以自适应地学习负斜率，通常比 Leaky ReLU 表现更好。
- **缺点：** 增加了模型的参数数量。

**Softmax 函数:**

- **数学形式：** $ σ(z)_j = \frac{e^{z_j}}{\sum^{K}_{k=1} e^{z_k}}$，其中 z 是一个包含 K 个元素的向量。
- **输出范围：** (0, 1)，且所有元素的和为 1。
- **特点：** 通常用于**多分类**问题的输出层，将每个类别的得分转换为概率分布。