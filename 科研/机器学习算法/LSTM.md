# 什么是LSTM？

LSTM 是一种特殊的 RNN 结构，专门为学习长期依赖关系而设计。

它通过独特的**记忆细胞（Memory Cell）**和**门机制（Gating Mechanism）**，有效地缓解了梯度消失问题，能够捕捉序列中的长期依赖关系。

LSTM 是一种循环神经网络，其核心思想是引入一个长期记忆单元（Cell State）来存储长期信息，通过一组精心设计的门控机制（包括输入门、遗忘门和输出门）对信息进行选择性更新或删除。这些门的作用可以形象地理解为“滤波器”，它们控制着信息流动的方式，决定了哪些信息应该被保留、遗忘或输出。

# LSTM的目标

LSTM 的主要目标是解决传统 RNN 的以下问题：

1. **梯度消失与梯度爆炸**：RNN 在长序列中无法有效传播梯度，导致模型难以学习长期依赖关系。
2. **长期记忆的捕获**：传统 RNN 只能记住最近的上下文，而 LSTM 可以捕获序列中的长期依赖关系。

# LSTM 的网络结构

LSTM 的核心是**细胞状态（Cell State）** 和 **隐藏状态（Hidden State）**，它们通过以下三种门机制控制信息流动：

## 1. 细胞状态（Cell State）

- 细胞状态类似于传送带，贯穿整个 LSTM 单元。
- 它可以选择性地保留或丢弃信息，是信息的主要存储器。

## 2. 门机制（Gating Mechanism）

LSTM 的门机制由三部分组成：

- **遗忘门（Forget Gate）**：决定哪些信息需要从细胞状态中遗忘。
- **输入门（Input Gate）**：决定哪些新的信息需要被写入细胞状态。
- **输出门（Output Gate）**：决定当前时刻的输出，以及哪些信息将从细胞状态流向隐藏状态。

这些门的计算通过 sigmoid 和 tanh 激活函数完成，范围控制在 [0,1] 或 [-1,1]。

# LSTM 的数学原理

LSTM 相比于普通 RNN 多了“遗忘门（forget gate）”、“输入门（input gate）”和“输出门（output gate）”三个门控单元，以及一个“候选记忆”（cell candidate）分支。一次前向传播中，LSTM 会先根据上一个时间步的隐藏状态和当前输入同时计算这三个门的激活值和候选记忆值，然后更新当前的**细胞状态**（cell state），最后根据细胞状态和输出门计算当前的**隐藏状态**（hidden state）。这种结构使得网络能够在长序列中保留或遗忘信息，从而缓解梯度消失问题

## 1. 符号说明

$x_t$：时刻 $t$ 的输入向量

$h_{t-1}$：上一时刻的隐藏状态（hidden state）

$c_{t-1}$：上一时刻的细胞状态（cell state）

$W_f, U_f, b_f$：遗忘门的权重矩阵和偏置

$W_i, U_i, b_i$：输入门的权重矩阵和偏置

$W_o, U_o, b_o$：输出门的权重矩阵和偏置

$W_c, U_c, b_c$：候选记忆的权重矩阵和偏置

$\sigma(\cdot)$：元素级 sigmoid 激活函数

$\tanh(\cdot)$：元素级双曲正切激活函数

## 2. 门和候选记忆的计算

在时刻 $t$，首先对遗忘门、输入门、输出门和候选记忆值进行线性变换并加偏置，然后分别通过激活函数得到各自输出：

1. **遗忘门**（决定前一状态 $c_{t-1}$ 中哪些信息需要遗忘）
   $$
   f_t = \sigma\bigl(W_f x_t + U_f h_{t-1} + b_f\bigr)
   $$

2. **输入门**（决定当前的候选信息中哪些需要写入细胞状态）
   $$
   i_t = \sigma\bigl(W_i x_t + U_i h_{t-1} + b_i\bigr)
   $$

3. **候选记忆**（对新信息做一次候选变换）
   $$
   \tilde c_t = \tanh\bigl(W_c x_t + U_c h_{t-1} + b_c\bigr)
   $$

4. **输出门**（决定当前细胞状态哪些部分会输出到隐藏状态）
   $$
   o_t = \sigma\bigl(W_o x_t + U_o h_{t-1} + b_o\bigr)
   $$

------

## 3. 细胞状态更新

利用遗忘门和输入门，对细胞状态进行“遗忘”与“写入”操作，得到新的细胞状态 $c_t$：
$$
c_t = f_t \odot c_{t-1} \;+\; i_t \odot \tilde c_t
$$
其中“$\odot$”表示逐元素乘积。

------

## 4. 隐藏状态计算

最后，通过输出门对新细胞状态做一次非线性映射，得到当前的隐藏状态 $h_t$：
$$
h_t = o_t \;\odot\; \tanh(c_t)
$$
隐藏状态 $h_t$ 通常也是 LSTM 在序列任务中对下一层或下一个时间步的输出来说的“记忆”或“上下文”信息。

------

## 5. 整体前向流程图

```
cpp


复制编辑
输入 x_t,  h_{t-1},  c_{t-1}
        │
        ├─遗忘门 f_t = σ(W_f x_t + U_f h_{t-1} + b_f)
        │
        ├─输入门 i_t = σ(W_i x_t + U_i h_{t-1} + b_i)
        │
        ├─候选记忆 ĉ_t = tanh(W_c x_t + U_c h_{t-1} + b_c)
        │
        ├─输出门 o_t = σ(W_o x_t + U_o h_{t-1} + b_o)
        │
        ├─细胞状态 c_t = f_t ⊙ c_{t-1} + i_t ⊙ ĉ_t
        │
        └─隐藏状态 h_t = o_t ⊙ tanh(c_t)
```